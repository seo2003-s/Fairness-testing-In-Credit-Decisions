{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEovHtUoTG6K"
   },
   "source": [
    "# CS108/212 STAT108/212 W25 Course Project\n",
    "\n",
    "### Team Details\n",
    "\n",
    "- Teammate 1: Kyle Russell\n",
    "- Teammate 2: Arhum Shahid \n",
    "- Teammate 3: Seona Magdum \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Milestone: Mitigating Bias\n",
    "For this project milestone, each teammate will implement bias mitigation strategies and assess pre and post bias mitigation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H61P2lQlNz1Q"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zcjl4O1GN24E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\golde\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from ucimlrepo) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\golde\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\golde\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# This is the same dataset we worked on in Discussion 3\n",
    "!pip install ucimlrepo\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tXKU9aa5HPd"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CRQN7QJF5JUH"
   },
   "outputs": [],
   "source": [
    "# Dataset imports\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "# For Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V3zrVf11GUF"
   },
   "source": [
    "# Loading dataset\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract data in the same way we did in Week 3 (Do this only once when coding as it sometimes takes a while)\n",
    "statlog_german_credit_data = fetch_ucirepo(id=144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Uimn7Sde1Jp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 1000\n",
      "No. of features: 20\n",
      "Group Counts: {3: 625, 0: 84, 2: 226, 1: 65}\n"
     ]
    }
   ],
   "source": [
    "## Use metadata and variables to find information about different attributes\n",
    "# print(statlog_german_credit_data.metadata)\n",
    "# print(statlog_german_credit_data.variables)\n",
    "\n",
    "# Feature and target labels\n",
    "X = statlog_german_credit_data.data.features\n",
    "y = statlog_german_credit_data.data.targets\n",
    "\n",
    "# Extract sensitive features from attributes\n",
    "gender = X[\"Attribute9\"].apply(lambda x: \"Male\" if x in [\"A91\", \"A93\", \"A94\"] else \"Female\")\n",
    "age_binary = X[\"Attribute13\"].apply(lambda x: \"Old\" if x >= 25 else \"Young\")\n",
    "\n",
    "# Create a mapping to map what sensitive feature class it belongs to\n",
    "group_labels = (gender == \"Male\").astype(int) + (age_binary == \"Old\").astype(int) * 2\n",
    "\n",
    "# Print some stats\n",
    "print(f\"No. of samples: {X.shape[0]}\")\n",
    "print(f\"No. of features: {X.shape[1]}\")\n",
    "print(f\"Group Counts: {dict(collections.Counter(group_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes for Group label Meanings**\n",
    "- 0: maps to (Female, Young)\n",
    "- 1: maps to (Male, Young)\n",
    "- 2: maps to (Female, Old)\n",
    "- 3: maps to (Male, Old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq2dMuD87D0f"
   },
   "source": [
    "# Preparing dataset\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Mq3wYB1H7CXM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples AFTER cleaning: 1000\n",
      "No. of features AFTER encoding: 46\n"
     ]
    }
   ],
   "source": [
    "# Some subset of following dataset preparation steps may be necessary depending on your dataset,\n",
    "# 1. Drop unnecessary features\n",
    "# 2. Handle missing data\n",
    "# 3. Encode categorical features\n",
    "# 4. Normalize numerical features\n",
    "# 5. Encode target (if your task is classification)\n",
    "\n",
    "# No unnecessary features or missing data in German Credit Dataset\n",
    "# Encode special protected categories\n",
    "X_dropped = X.drop(columns = [\"Attribute9\", \"Attribute13\"]).copy()\n",
    "\n",
    "X_protect = X[[\"Attribute9\", \"Attribute13\"]].copy()\n",
    "X_protect[\"Attribute9\"] = X_protect[\"Attribute9\"].apply(lambda x: 1 if x in [\"A91\", \"A93\", \"A94\"] else 0).astype(bool)\n",
    "X_protect[\"Attribute13\"] = X_protect[\"Attribute13\"].apply(lambda x: 1 if x >= 25 else 0).astype(bool)\n",
    "\n",
    "# Encode remaining categorical features\n",
    "X_dropped = pd.get_dummies(X_dropped, drop_first=True) # Drop first of each hot one encoding to avoid multicollinearity\n",
    "X_encoded = pd.concat([X_dropped, X_protect], axis=1)\n",
    "\n",
    "# Normalize numerical features\n",
    "num_cols = X_encoded.select_dtypes(include=['number']).columns\n",
    "scaler = StandardScaler() # Converts to z scores (how many sd a value is from the mean)\n",
    "X_encoded[num_cols] = scaler.fit_transform(X_encoded[num_cols])\n",
    "\n",
    "# Encode the target variable\n",
    "X_cleaned = X_encoded.copy()\n",
    "y_cleaned = y[\"class\"].apply(lambda x: 1 if x == 1 else 0).copy() # Note that class defines 1 as good and 2 as bad\n",
    "\n",
    "# Note: X and y have been modified before the following lines of code!\n",
    "print(f\"No. of samples AFTER cleaning: {X_cleaned.shape[0]}\")\n",
    "assert X_cleaned.shape[0] == y_cleaned.shape[0] == group_labels.shape[0] ## Ensure that the target and group_labels have been updated if some samples were removed during cleaning.\n",
    "print(f\"No. of features AFTER encoding: {X_cleaned.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1xfwjUT3nJ0"
   },
   "source": [
    "# Getting training and testing sets\n",
    "\n",
    "Note: Train-test split is made **ONCE** to obtain the _training set_ and the _testing set_ and every teammate will use the training set to train their baseline model and test the trained model using the testing set. **NEVER** modify the testing set once it has been created.\n",
    "Therefore, the following code cell does not need to be edited.\n",
    "\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "udqlgotu5a5m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training samples: 800\n",
      "No. of testing samples: 200\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, \\\n",
    "y_train, y_test, \\\n",
    "group_labels_train, group_labels_test = train_test_split(X_cleaned, y_cleaned, group_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"No. of training samples: {X_train.shape[0]}\")\n",
    "print(f\"No. of testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Changed since we are modifying copies of the original dataset with X_cleaned and y_cleaned\n",
    "# # Delete X, y and group_label variables to make sure they are not used later on.\n",
    "del X\n",
    "del y\n",
    "del group_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stfLke4NBA-B"
   },
   "source": [
    "# Setting up evaluation metrics\n",
    "Note: The same evaluation function will be used by all teammates.\n",
    "\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RxX61lMDA50u"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred, g_labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the performance of your trained model on the testing set.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : array-like\n",
    "        The true labels of the testing set.\n",
    "    y_pred : array-like\n",
    "        The predicted labels of the testing set.\n",
    "    g_labels : array-like\n",
    "        The group labels of the testing set.\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        A dictionary containing the evaluation results.\n",
    "  \n",
    "        Example:\n",
    "          For classification task, the task-specific performance metrics like {'accuracy': <value>, 'f1_score': <value>, ...}\n",
    "          and fairness metrics like {'demographic_parity': <value>, 'equalized_odds': <value>, ...}.\n",
    "  \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "  \n",
    "    # Note: These metrics will be calculated for - 1. the full testing set, 2. individual groups.\n",
    "    # Task-specific performance metrics\n",
    "    TP = np.sum((y_test == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_test == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_test == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_test == 1) & (y_pred == 0)) \n",
    "\n",
    "    results[\"accuracy\"] = (TP + TN) / (TP + TN + FP + FN)\n",
    "    results[\"precision\"] = TP / (TP + FP)\n",
    "    results[\"recall\"] = TP / (TP + FN)\n",
    "    \n",
    "    # Fairness metric\n",
    "    unique_groups = np.unique(g_labels) # 0, 1, 2, 3\n",
    "    f_m = {}\n",
    "\n",
    "    for group in unique_groups:\n",
    "        y_test_g = y_test[g_labels == group]\n",
    "        y_pred_g = y_pred[g_labels == group]\n",
    "        \n",
    "        TP = np.sum((y_test_g == 1) & (y_pred_g == 1))   \n",
    "        TN = np.sum((y_test_g == 0) & (y_pred_g == 0))\n",
    "        FP = np.sum((y_test_g == 0) & (y_pred_g == 1))\n",
    "        FN = np.sum((y_test_g == 1) & (y_pred_g == 0))\n",
    "\n",
    "        f_m[group] = {\n",
    "            \"d_p\": (TP + FP) / (TP + TN + FP + FN), # Demographic parity\n",
    "            \"a_p\": (TP + TN) / (TP + TN + FP + FN), # Accuracy parity (accuracy for each group)\n",
    "            \"p_p\": TP / (TP + FP), # Predictive parity\n",
    "            \"e_e\": TP / (TP + FN), # Equal Opportunity\n",
    "        }\n",
    "        \n",
    "    \"\"\"    \n",
    "    For a single result, we will use the ratio between genders and age for each metric (closer to 1 -> more fair)\n",
    "    Note: each group corresponds to the categories\n",
    "    \n",
    "    0: maps to (Female, Young)\n",
    "    1: maps to (Male, Young)\n",
    "    2: maps to (Female, Old)\n",
    "    3: maps to (Male, Old)\n",
    "\n",
    "    Gender will take the ratio between Female : Male\n",
    "    Age will take the ratio between Young : Old\n",
    "    \"\"\"\n",
    "\n",
    "    # ratio < 1: means Female parities and opportunities less often (and viseversa) \n",
    "    results[\"demographic_parity_gender_ratio\"] = (f_m[0][\"d_p\"] + f_m[2][\"d_p\"]) / (f_m[1][\"d_p\"] + f_m[3][\"d_p\"])\n",
    "    results[\"accuracy_parity_gender_ratio\"] = (f_m[0][\"a_p\"] + f_m[2][\"a_p\"]) / (f_m[1][\"a_p\"] + f_m[3][\"a_p\"])\n",
    "    results[\"predictive_parity_gender_ratio\"] = (f_m[0][\"p_p\"] + f_m[2][\"p_p\"]) / (f_m[1][\"p_p\"] + f_m[3][\"p_p\"])\n",
    "    results[\"equal_opportunity_gender_ratio\"] = (f_m[0][\"e_e\"] + f_m[2][\"e_e\"]) / (f_m[1][\"e_e\"] + f_m[3][\"e_e\"])\n",
    "\n",
    "    # ratio < 1: means Young parities and opportunities less often (and viseversa) \n",
    "    results[\"demographic_parity_age_ratio\"] = (f_m[0][\"d_p\"] + f_m[1][\"d_p\"]) / (f_m[2][\"d_p\"] + f_m[3][\"d_p\"])\n",
    "    results[\"accuracy_parity_age_ratio\"] = (f_m[0][\"a_p\"] + f_m[1][\"a_p\"]) / (f_m[2][\"a_p\"] + f_m[3][\"a_p\"])\n",
    "    results[\"predictive_parity_age_ratio\"] = (f_m[0][\"p_p\"] + f_m[1][\"p_p\"]) / (f_m[2][\"p_p\"] + f_m[3][\"p_p\"])\n",
    "    results[\"equal_opportunity_age_ratio\"] = (f_m[0][\"e_e\"] + f_m[1][\"e_e\"]) / (f_m[2][\"e_e\"] + f_m[3][\"e_e\"])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My_giZbvpFEK"
   },
   "source": [
    "# Training baseline models (INDIVIDUAL CONTRIBUTION)\n",
    "_(minor modifications from previous milestone)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kK2P9uZnIKGf"
   },
   "outputs": [],
   "source": [
    "## A place to save all teammates's baseline results\n",
    "all_baseline_results = [] ## DO NOT EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "400FpWHZ_z0S"
   },
   "source": [
    "## Teammate 1: Kyle Russell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BjKUAk4I_4DQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.75500\n",
      "precision: 0.80667\n",
      "recall: 0.85816\n",
      "demographic_parity_gender_ratio: 0.95376\n",
      "accuracy_parity_gender_ratio: 0.93264\n",
      "predictive_parity_gender_ratio: 0.82464\n",
      "equal_opportunity_gender_ratio: 0.93903\n",
      "demographic_parity_age_ratio: 0.63285\n",
      "accuracy_parity_age_ratio: 0.92268\n",
      "predictive_parity_age_ratio: 1.04006\n",
      "equal_opportunity_age_ratio: 0.72044\n"
     ]
    }
   ],
   "source": [
    "# Select a model and train it on the training set\n",
    "model = DecisionTreeClassifier(max_depth=5, min_samples_split=15, min_samples_leaf=2, random_state=135) # Adjustable hyperparameters\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.5f}\")\n",
    "    \n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Kyle Russell'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = DecisionTreeClassifier(max_depth=5, min_samples_split=15, min_samples_leaf=2, random_state=135)\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VYWPshD_zlw"
   },
   "source": [
    "## Teammate 2: Arhum Shahid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dgDw0Ta7_7FT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.68000\n",
      "precision: 0.72000\n",
      "recall: 0.89362\n",
      "demographic_parity_gender_ratio: 0.98571\n",
      "accuracy_parity_gender_ratio: 0.95455\n",
      "predictive_parity_gender_ratio: 0.85990\n",
      "equal_opportunity_gender_ratio: 1.03240\n",
      "demographic_parity_age_ratio: 0.82096\n",
      "accuracy_parity_age_ratio: 1.00000\n",
      "predictive_parity_age_ratio: 1.00566\n",
      "equal_opportunity_age_ratio: 0.91338\n"
     ]
    }
   ],
   "source": [
    "# Select a model and train it on the training set\n",
    "model = KNeighborsClassifier(n_neighbors=13) # Adjustable hyperparameter\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.5f}\")\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Arhum Shahid'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = KNeighborsClassifier(n_neighbors=13)\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Bq--e_8_7o2"
   },
   "source": [
    "## Teammate 3: Seona Magdum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "j9DspEoL_-EV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.79000\n",
      "precision: 0.82781\n",
      "recall: 0.88652\n",
      "demographic_parity_gender_ratio: 0.99408\n",
      "accuracy_parity_gender_ratio: 1.08377\n",
      "predictive_parity_gender_ratio: 0.93232\n",
      "equal_opportunity_gender_ratio: 1.11170\n",
      "demographic_parity_age_ratio: 0.65196\n",
      "accuracy_parity_age_ratio: 1.00000\n",
      "predictive_parity_age_ratio: 1.13472\n",
      "equal_opportunity_age_ratio: 0.83460\n"
     ]
    }
   ],
   "source": [
    "# Select a model and train it on the training set\n",
    "model = LogisticRegression(solver='liblinear',random_state=130)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.5f}\")\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Seona Magdum'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = LogisticRegression(solver='liblinear',random_state=130)\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Lq5X0Xb5eWA"
   },
   "source": [
    "# Mitigating Bias (INDIVIDUAL CONTRIBUTION)\n",
    "\n",
    "_(new in this milestone)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LwfT5Uv9I0j6"
   },
   "outputs": [],
   "source": [
    "## A place to save all teammates' post-mitigation results\n",
    "all_mitigated_results = [] ## DO NOT EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmQrfFvn6E85"
   },
   "source": [
    "## Teammate 1: Kyle Russell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CZhAOHFB5lzh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.75500\n",
      "precision: 0.76744\n",
      "recall: 0.93617\n",
      "demographic_parity_gender_ratio: 1.11765\n",
      "accuracy_parity_gender_ratio: 0.88500\n",
      "predictive_parity_gender_ratio: 0.80245\n",
      "equal_opportunity_gender_ratio: 1.09515\n",
      "demographic_parity_age_ratio: 0.91150\n",
      "accuracy_parity_age_ratio: 0.97382\n",
      "predictive_parity_age_ratio: 0.98230\n",
      "equal_opportunity_age_ratio: 0.98257\n"
     ]
    }
   ],
   "source": [
    "## Mitigation method (reweighing)\n",
    "# Count the weights per group\n",
    "group_counts = collections.Counter(group_labels_train)\n",
    "\n",
    "# Compute inverse weights (Smaller counts should have larger weights)\n",
    "group_inverse_weights = {}\n",
    "for group, count in group_counts.items():\n",
    "    group_inverse_weights[group] = 1.0 / count\n",
    "\n",
    "# Rescale weights for stability (scaling up to sample size)\n",
    "scaling_factor = len(group_labels_train) / sum(group_inverse_weights.values())\n",
    "group_scaled_weights = {group: weight * scaling_factor for group, weight in group_inverse_weights.items()}\n",
    "\n",
    "# Apply these weights to each row in our dataset\n",
    "sample_weights = []\n",
    "for group in group_labels_train:\n",
    "    sample_weights.append(group_scaled_weights[group])\n",
    "\n",
    "# Select a model and train it on the training set\n",
    "model_mitigated = DecisionTreeClassifier(max_depth=5, min_samples_split=15, min_samples_leaf=2, random_state=135) \n",
    "model_mitigated.fit(X_train, y_train, sample_weight=sample_weights) # We are using the reweighted values for each group\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = model_mitigated.predict(X_test)\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Kyle Russell'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = DecisionTreeClassifier(max_depth=5, min_samples_split=15, min_samples_leaf=2, random_state=135)\n",
    "results_mitigated['mitigation_strategy'] = 'preprocessing: Reweighting'\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "for metric, value in results_mitigated.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X05x8kjr6KrH"
   },
   "source": [
    "### Teammate 1's Conclusions\n",
    "\n",
    "&emsp;The reweighting strategy improved fairness by bringing parity values closer to 1, while maintaining an overall accuracy at 75.5%. We saw that age related parity comparisons improved significantly, while gender-related parity saw mixed results (although age was more imbalanced outcome in this dataset before). Looking at each of the categories:\n",
    "\n",
    "#### Gender-Based Fairness Adjustments\n",
    "- **Demographic Parity** increased from **0.95376 → 1.11765**, which maintains an acceptable range (though further from 1, favoring females).  \n",
    "- **Equal Opportunity** shifted from **0.93903 → 1.09414**, which is also in an acceptable range but slightly further from a ratio of 1.  \n",
    "\n",
    "#### Age-Based Fairness Adjustments  \n",
    "- **Demographic Parity** improved from **0.63285 → 0.91154**, moving **44% closer to 1** (a significant improvement).  \n",
    "- **Equal Opportunity** moved from **0.72044 → 0.98257**, making it **36% closer to 1**.  \n",
    "\n",
    "#### Performance Trade-Offs  \n",
    "- **Recall improved** from **0.85816 → 0.93617** (**+9.1%**), meaning better detection of positive cases.  \n",
    "- **Precision decreased** from **0.85816 → 0.76744** (**-4.9%**), meaning slightly more false positives.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLB2ggUCBen_"
   },
   "source": [
    "## Teammate 2: Arhum Shahid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vtbctHpBBgna"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.66500\n",
      "precision: 0.80328\n",
      "recall: 0.69504\n",
      "demographic_parity_gender_ratio: 0.82979\n",
      "accuracy_parity_gender_ratio: 0.98039\n",
      "predictive_parity_gender_ratio: 0.85572\n",
      "equal_opportunity_gender_ratio: 0.84402\n",
      "demographic_parity_age_ratio: 0.64331\n",
      "accuracy_parity_age_ratio: 0.80357\n",
      "predictive_parity_age_ratio: 0.90306\n",
      "equal_opportunity_age_ratio: 0.63645\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "# Function to perform massaging strategy\n",
    "def apply_massaging(X, y, group_labels):\n",
    "    \"\"\"Applies the massaging technique to reduce bias in training data.\"\"\"\n",
    "    enn = EditedNearestNeighbours()\n",
    "    X_resampled, y_resampled = enn.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply massaging to training data\n",
    "X_train_massaged, y_train_massaged = apply_massaging(X_train, y_train, group_labels_train)\n",
    "\n",
    "# Train a new KNN model on massaged data\n",
    "knn_massaged = KNeighborsClassifier(n_neighbors=13)\n",
    "knn_massaged.fit(X_train_massaged, y_train_massaged)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mitigated = knn_massaged.predict(X_test)\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save results\n",
    "results_mitigated['teammate'] = 'Arhum Shahid'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = KNeighborsClassifier(n_neighbors=13)\n",
    "results_mitigated['mitigation_strategy'] = 'preprocessing: Massaging'\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "\n",
    "for metric, value in results_mitigated.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmwKk9f2BlaM"
   },
   "source": [
    "### Teammate 2's Conclusions\n",
    "The massaging strategy resulted in improvements in fairness while maintaining an acceptable trade-off in performance. After applying massaging, the model's accuracy shifted from **0.645** (baseline) to **0.665**, reflecting a **3.1%** improvement. Gender-based fairness metrics showed a slight improvement, with demographic parity increasing to **0.8298**, and equal opportunity reaching **0.8440**. Age-based fairness also improved, particularly in equal opportunity, which increased to **0.6365**. While precision slightly decreased to **0.8033**, recall improved to **0.6950**, indicating fewer false negatives. This suggests that the massaging strategy effectively helped reduce bias in the dataset, leading to a fairer decision-making process while keeping predictive performance relatively stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUpxtxuUCF_I"
   },
   "source": [
    "## Teammate 3: Seona Magdum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4KlNlq7lCInQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mitigated Model Evaluation:\n",
      "accuracy: 0.77000\n",
      "precision: 0.78443\n",
      "recall: 0.92908\n",
      "demographic_parity_gender_ratio: 1.00508\n",
      "accuracy_parity_gender_ratio: 0.91304\n",
      "predictive_parity_gender_ratio: 0.82847\n",
      "equal_opportunity_gender_ratio: 1.01568\n",
      "demographic_parity_age_ratio: 0.77130\n",
      "accuracy_parity_age_ratio: 1.04124\n",
      "predictive_parity_age_ratio: 1.06905\n",
      "equal_opportunity_age_ratio: 0.90737\n"
     ]
    }
   ],
   "source": [
    "# Implement your bias mitigation strategy: Regularization\n",
    "model_mitigated = LogisticRegression(penalty='l2',C=0.1,solver='liblinear',random_state=130)\n",
    "model_mitigated.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = model_mitigated.predict(X_test)\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Seona Magdum'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = LogisticRegression(penalty='l2',C=0.1,solver='liblinear',random_state=130)\n",
    "results_mitigated['mitigation_strategy'] = 'inprocessing: Regularization'\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "\n",
    "print(\"\\nMitigated Model Evaluation:\")\n",
    "for metric, value in results_mitigated.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YN7oELoCKmF"
   },
   "source": [
    "### Teammate 3's Conclusions\n",
    "We can see that this reweighting srategy has improved fairness for most of the fairness metrics. The accuracy has decreased from about 2.5% from 79% to 77% while the precision has a five percent decrease from 82% to 78%. However, recall increased significantly by 4%, from 88% to 92%, meaning a fewer number of false negatives. <br>\n",
    "For gender, the equal oppurtunity and demographic parity became slightly fairer. Equal oppurtunity dropped from 1.11 to 1.01, suggesting a more balanced ratio. Demographic parity also slightly increased, from 0.99 to 1.001. The accuracy parity and predictive parity decreased in number and became further away from 1, meaning both the prediction and accuracy consistency across gender groups slightly worsened. <br>\n",
    "For age, the demographic parity, predictive parity and equal oppurtunity all became more fair, with accuracy parity having a slight increase from 1 to 1.04. The Demographic parity had the most improvement from 0.65 to 0.77 with our model, as well as equal oppurtunity improving by around 0.07. <br>\n",
    "Overall, after mitigation the fairness improved for both gender and age. Accuracy and precison slightly dropped, with the recall increasing, suggesting a more inclusive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4LK5WRD7LvV"
   },
   "source": [
    "# Conclusions\n",
    "_(new in this milestone)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ZAgZJYf7F70N",
    "outputId": "42dcf95b-699c-4037-bb88-2c604648d111"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teammate</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>predictor_model</th>\n",
       "      <th>mitigation_strategy</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>demographic_parity_gender_ratio</th>\n",
       "      <th>accuracy_parity_gender_ratio</th>\n",
       "      <th>predictive_parity_gender_ratio</th>\n",
       "      <th>equal_opportunity_gender_ratio</th>\n",
       "      <th>demographic_parity_age_ratio</th>\n",
       "      <th>accuracy_parity_age_ratio</th>\n",
       "      <th>predictive_parity_age_ratio</th>\n",
       "      <th>equal_opportunity_age_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kyle Russell</td>\n",
       "      <td>baseline</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=5, min_sample...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.858156</td>\n",
       "      <td>0.953757</td>\n",
       "      <td>0.932642</td>\n",
       "      <td>0.824639</td>\n",
       "      <td>0.939033</td>\n",
       "      <td>0.632850</td>\n",
       "      <td>0.922680</td>\n",
       "      <td>1.040056</td>\n",
       "      <td>0.720440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arhum Shahid</td>\n",
       "      <td>baseline</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=13)</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.859897</td>\n",
       "      <td>1.032404</td>\n",
       "      <td>0.820961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.005657</td>\n",
       "      <td>0.913381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seona Magdum</td>\n",
       "      <td>baseline</td>\n",
       "      <td>LogisticRegression(random_state=130, solver='l...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.827815</td>\n",
       "      <td>0.886525</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.083770</td>\n",
       "      <td>0.932322</td>\n",
       "      <td>1.111704</td>\n",
       "      <td>0.651961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.134721</td>\n",
       "      <td>0.834603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kyle Russell</td>\n",
       "      <td>post-mitigation</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=5, min_sample...</td>\n",
       "      <td>preprocessing: Reweighting</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.802455</td>\n",
       "      <td>1.095149</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.973822</td>\n",
       "      <td>0.982298</td>\n",
       "      <td>0.982568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arhum Shahid</td>\n",
       "      <td>post-mitigation</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=13)</td>\n",
       "      <td>preprocessing: Massaging</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.695035</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.844021</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.903061</td>\n",
       "      <td>0.636451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seona Magdum</td>\n",
       "      <td>post-mitigation</td>\n",
       "      <td>LogisticRegression(C=0.1, random_state=130, so...</td>\n",
       "      <td>inprocessing: Regularization</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.929078</td>\n",
       "      <td>1.005076</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.828466</td>\n",
       "      <td>1.015682</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>1.041237</td>\n",
       "      <td>1.069046</td>\n",
       "      <td>0.907371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       teammate  experiment_type  \\\n",
       "0  Kyle Russell         baseline   \n",
       "1  Arhum Shahid         baseline   \n",
       "2  Seona Magdum         baseline   \n",
       "0  Kyle Russell  post-mitigation   \n",
       "1  Arhum Shahid  post-mitigation   \n",
       "2  Seona Magdum  post-mitigation   \n",
       "\n",
       "                                     predictor_model  \\\n",
       "0  DecisionTreeClassifier(max_depth=5, min_sample...   \n",
       "1               KNeighborsClassifier(n_neighbors=13)   \n",
       "2  LogisticRegression(random_state=130, solver='l...   \n",
       "0  DecisionTreeClassifier(max_depth=5, min_sample...   \n",
       "1               KNeighborsClassifier(n_neighbors=13)   \n",
       "2  LogisticRegression(C=0.1, random_state=130, so...   \n",
       "\n",
       "            mitigation_strategy  accuracy  precision    recall  \\\n",
       "0                          NONE     0.755   0.806667  0.858156   \n",
       "1                          NONE     0.680   0.720000  0.893617   \n",
       "2                          NONE     0.790   0.827815  0.886525   \n",
       "0    preprocessing: Reweighting     0.755   0.767442  0.936170   \n",
       "1      preprocessing: Massaging     0.665   0.803279  0.695035   \n",
       "2  inprocessing: Regularization     0.770   0.784431  0.929078   \n",
       "\n",
       "   demographic_parity_gender_ratio  accuracy_parity_gender_ratio  \\\n",
       "0                         0.953757                      0.932642   \n",
       "1                         0.985714                      0.954545   \n",
       "2                         0.994083                      1.083770   \n",
       "0                         1.117647                      0.885000   \n",
       "1                         0.829787                      0.980392   \n",
       "2                         1.005076                      0.913043   \n",
       "\n",
       "   predictive_parity_gender_ratio  equal_opportunity_gender_ratio  \\\n",
       "0                        0.824639                        0.939033   \n",
       "1                        0.859897                        1.032404   \n",
       "2                        0.932322                        1.111704   \n",
       "0                        0.802455                        1.095149   \n",
       "1                        0.855721                        0.844021   \n",
       "2                        0.828466                        1.015682   \n",
       "\n",
       "   demographic_parity_age_ratio  accuracy_parity_age_ratio  \\\n",
       "0                      0.632850                   0.922680   \n",
       "1                      0.820961                   1.000000   \n",
       "2                      0.651961                   1.000000   \n",
       "0                      0.911504                   0.973822   \n",
       "1                      0.643312                   0.803571   \n",
       "2                      0.771300                   1.041237   \n",
       "\n",
       "   predictive_parity_age_ratio  equal_opportunity_age_ratio  \n",
       "0                     1.040056                     0.720440  \n",
       "1                     1.005657                     0.913381  \n",
       "2                     1.134721                     0.834603  \n",
       "0                     0.982298                     0.982568  \n",
       "1                     0.903061                     0.636451  \n",
       "2                     1.069046                     0.907371  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define column order with metadata fields first\n",
    "column_order = [\n",
    "    \"teammate\", \"experiment_type\", \"predictor_model\", \"mitigation_strategy\", \n",
    "    \"accuracy\", \"precision\", \"recall\", \n",
    "    \"demographic_parity_gender_ratio\", \"accuracy_parity_gender_ratio\", \"predictive_parity_gender_ratio\", \"equal_opportunity_gender_ratio\", \n",
    "    \"demographic_parity_age_ratio\", \"accuracy_parity_age_ratio\", \"predictive_parity_age_ratio\", \"equal_opportunity_age_ratio\"\n",
    "]\n",
    "\n",
    "# Collect all the results in one table.\n",
    "overall_results = pd.concat([pd.DataFrame(all_baseline_results), pd.DataFrame(all_mitigated_results)])\n",
    "overall_results = overall_results[column_order]\n",
    "overall_results ## Note: The table displayed below in this starter notebook is for your reference, your team's table will be slightly different (e.g. different metrics, no.of sensitive attribute-based groups, actual values, etc.) upon successful completion of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds54O8vGGAEw"
   },
   "source": [
    "&emsp;In terms of accuracy, precision and recall (performance metrics), baseline models generally had the edge compared to their post mitigation counterparts. Post mitigation models saw slight drops in accuracy (or at best maintained their accuracy) which suggests common fairness performance trade-offs. In terms of fairness and bias mitigation, certain models generally improved by the different bias mitigation methods. However, a more local model such as KNN struggled to maintain balance after applying massaging. Much of the models had improved in terms of metrics such as demographic parity age ratio and equal opportunity age ratio (excluding KNN), but struggled in improving the fairness metrics for gender ratio, which may have been the case as gender was relatively balanced before. In terms of overall performance, Decision Trees with reweighing and logistic regression with regularization are the best choices. However, if performance is the priority, the baseline Logistic regression performs well, at the cost of some fairness gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-8NLvIOsPL-"
   },
   "source": [
    "# References\n",
    "- Kyle Russell:\n",
    "   - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit \n",
    "   - https://arxiv.org/html/2312.12560v1\n",
    "   - https://docs.python.org/3/library/collections.html\n",
    "- Arhum Shahid:\n",
    "   - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.EditedNearestNeighbours.html\n",
    "- Seona Magdum:\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV3LsIXqKAg1"
   },
   "source": [
    "# Disclosures\n",
    "\n",
    "- Use of chatgpt to look up how to use different functions of pandas, numpy, seaborn, and other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7V3zrVf11GUF",
    "zq2dMuD87D0f",
    "h1xfwjUT3nJ0",
    "stfLke4NBA-B",
    "400FpWHZ_z0S",
    "_VYWPshD_zlw",
    "_Bq--e_8_7o2",
    "DXm38f8ZABN3",
    "ZLB2ggUCBen_",
    "ZUpxtxuUCF_I",
    "3qNMlhiECMcD"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
